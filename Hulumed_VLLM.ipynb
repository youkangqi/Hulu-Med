{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b83a9-f8a6-4e3b-aa42-4c426c31cf99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "# HuluMed Multi-Modal Test Suite\n",
    "Test HuluMed model's multimodal capabilities in Jupyter Notebook\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Import required libraries\n",
    "import os\n",
    "#os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "import sys\n",
    "\n",
    "# Add hulu_utils to path\n",
    "from vllm.model_executor.models.hulu_utils import load_images, load_video, load_3d\n",
    "\n",
    "# %%\n",
    "MODEL_PATH = \"ZJU-AI4H/Hulu-Med-7B\"\n",
    "VIDEO_PATH = \"./demo.mp4\"\n",
    "IMAGE_PATH = \"./demo.jpg\"\n",
    "NII_PATH = \"./demo.nii\"\n",
    "\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "\n",
    "print(\"‚úÖ Environment configured\")\n",
    "print(f\"üìÅ Model path: {MODEL_PATH}\")\n",
    "print(f\"üé¨ Video path: {VIDEO_PATH}\")\n",
    "print(f\"üñºÔ∏è  Image path: {IMAGE_PATH}\")\n",
    "print(f\"üè• NII path: {NII_PATH}\")\n",
    "\n",
    "# %%\n",
    "# Initialize model (run once)\n",
    "print(\"üöÄ Loading model...\")\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"bfloat16\",\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    limit_mm_per_prompt={\"image\": 512},\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# %%\n",
    "# Test 1: Single image test\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 1: Single Image\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if os.path.exists(IMAGE_PATH):\n",
    "    images = load_images(IMAGE_PATH)\n",
    "    conversation = [{\"role\": \"user\", \"content\": \"<image>Describe this image.\"}]\n",
    "    prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    vllm_input = {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": images}\n",
    "    }\n",
    "    \n",
    "    sampling_params = SamplingParams(temperature=0.0, max_tokens=256)\n",
    "    outputs = llm.generate([vllm_input], sampling_params)\n",
    "    \n",
    "    print(f\"üì∏ Input: Single image from {IMAGE_PATH}\")\n",
    "    print(f\"üí¨ Question: Describe this image.\")\n",
    "    print(f\"ü§ñ Answer: {outputs[0].outputs[0].text}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Image not found: {IMAGE_PATH}\")\n",
    "\n",
    "# %%\n",
    "# Test 2: Text-only test\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 2: Text Only\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "conversation = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "vllm_input = {\"prompt\": prompt}\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=256)\n",
    "outputs = llm.generate([vllm_input], sampling_params)\n",
    "\n",
    "print(f\"üí¨ Question: What is the capital of France?\")\n",
    "print(f\"ü§ñ Answer: {outputs[0].outputs[0].text}\")\n",
    "\n",
    "# %%\n",
    "# Test 3: Multiple images test\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 3: Multiple Images\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if os.path.exists(IMAGE_PATH):\n",
    "    # Load the same image twice as an example\n",
    "    images = load_images(IMAGE_PATH) * 2\n",
    "    conversation = [{\"role\": \"user\", \"content\": \"<image><image>Compare these two images.\"}]\n",
    "    prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    vllm_input = {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": images}\n",
    "    }\n",
    "    \n",
    "    sampling_params = SamplingParams(temperature=0.0, max_tokens=256)\n",
    "    outputs = llm.generate([vllm_input], sampling_params)\n",
    "    \n",
    "    print(f\"üì∏ Input: {len(images)} images\")\n",
    "    print(f\"üí¨ Question: Compare these two images.\")\n",
    "    print(f\"ü§ñ Answer: {outputs[0].outputs[0].text}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Image not found: {IMAGE_PATH}\")\n",
    "\n",
    "# %%\n",
    "# Test 4: 3D medical volume test\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 4: 3D Medical Volume\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if os.path.exists(NII_PATH):\n",
    "    slices = load_3d(NII_PATH, num_slices=32, axis=2)\n",
    "    print(f\"üìä Loaded {len(slices)} slices from 3D volume\")\n",
    "    \n",
    "    conversation = [{\"role\": \"user\", \"content\": \"<image>\"*len(slices) + \"Describe this 3D scan.\"}]\n",
    "    prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    vllm_input = {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": slices}\n",
    "    }\n",
    "    \n",
    "    sampling_params = SamplingParams(temperature=0.0, max_tokens=512)\n",
    "    outputs = llm.generate([vllm_input], sampling_params)\n",
    "    \n",
    "    print(f\"üè• Input: {len(slices)} slices from {NII_PATH}\")\n",
    "    print(f\"üí¨ Question: Describe this CT/MRI scan.\")\n",
    "    print(f\"ü§ñ Answer: {outputs[0].outputs[0].text}\")\n",
    "    \n",
    "    # Optional: Display first slice\n",
    "    #from IPython.display import display\n",
    "    #display(slices[0])\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  NII file not found: {NII_PATH}\")\n",
    "\n",
    "# %%\n",
    "# Test 5: Video test\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 5: Video\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if os.path.exists(VIDEO_PATH):\n",
    "    frames = load_video(VIDEO_PATH, fps=1.0, max_frames=64)\n",
    "    print(f\"üé¨ Loaded {len(frames)} frames from video\")\n",
    "    \n",
    "    conversation = [{\"role\": \"user\", \"content\": \"<image>\"*len(frames) + \"Describe this video.\"}]\n",
    "    prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    vllm_input = {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": frames}\n",
    "    }\n",
    "    \n",
    "    sampling_params = SamplingParams(temperature=0.0, max_tokens=512)\n",
    "    outputs = llm.generate([vllm_input], sampling_params)\n",
    "    \n",
    "    print(f\"üé• Input: {len(frames)} frames from {VIDEO_PATH}\")\n",
    "    print(f\"üí¨ Question: Describe this video.\")\n",
    "    print(f\"ü§ñ Answer: {outputs[0].outputs[0].text}\")\n",
    "    \n",
    "    # Optional: Display first frame\n",
    "    #from IPython.display import display\n",
    "    #display(frames[0])\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Video not found: {VIDEO_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ All tests completed!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "üí° Tips:\n",
    "- Each cell can be run independently\n",
    "- Modify custom_question and paths for custom tests\n",
    "- Adjust sampling_params to change generation strategy\n",
    "- Use display() to show images\n",
    "\"\"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
